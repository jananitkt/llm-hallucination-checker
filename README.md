
# ğŸ§  LLM Hallucination Checker

A full-stack AI reliability tool created by **Janani Thamilarasu** to help detect and verify potential hallucinations in large language model (LLM) outputs using OpenRouter and Wikipedia APIs.

---

## ğŸ” What Is a Hallucination?

A hallucination occurs when an LLM generates **false or unverifiable facts** with confidence.

This app:
- Accepts a user prompt
- Sends it to an LLM (via OpenRouter)
- Extracts named entities from the response
- Verifies them using the Wikipedia API
- Flags any mismatches

---

## ğŸ¯ Why This Project Matters

- Fact-checking is critical in AI safety and reliability.
- Hallucinations in fields like healthcare, law, or education can be dangerous.
- This tool demonstrates how AI + APIs can be used to detect false claims.

---

## âœ… Key Features

- ğŸ”‘ Uses OpenRouter for LLM responses
- ğŸŒ Uses Wikipedia REST API for fact-checking
- ğŸ” Named Entity Extraction (capitalized phrases)
- ğŸ“„ Highlights real vs. potentially made-up information
- ğŸ§‘â€ğŸ“ Designed for research, learning, and transparency

---

## ğŸš€ How to Run This Project

### 1. Install Node packages
```bash
npm install
```

### 2. Add your OpenRouter key to `.env`
Create a file named `.env` and add:
```env
OPENROUTER_API_KEY=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
```

### 3. Start the app
```bash
npm start
```

### 4. Visit:
[http://localhost:3000](http://localhost:3000)

---

## ğŸ§ª 3 Sample Prompts to Test

### 1. Political Check
```
Who was the president of India in 2023?
```
- âŒ LLM might say: Narendra Modi
- âœ… Wikipedia says: Droupadi Murmu

---

### 2. Movie Mismatch
```
Who played the lead role in the 2020 movie Interstellar?
```
- âŒ Might mix up years or actors
- âœ… Wikipedia confirms: Interstellar was 2014, Matthew McConaughey

---

### 3. Scientific Claim
```
Can humans breathe underwater without any assistance?
```
- âŒ Some models might say â€œyes with practiceâ€
- âœ… Reality: No, artificial assistance is needed

---

## ğŸ§  Educational Goals

- Learn to evaluate AI outputs
- Use real-world APIs to validate claims
- Build safer, more trustworthy LLM apps

---

## ğŸ“ Folder Structure

```
hallucination-checker/
â”œâ”€â”€ .env              â† contains OpenRouter API key (do not upload)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ package.json
â”œâ”€â”€ server.mjs        â† Node.js backend
â””â”€â”€ public/
    â”œâ”€â”€ index.html    â† Frontend UI
    â””â”€â”€ script.js     â† Browser logic
```

---

Built with â¤ï¸ by [Janani Thamilarasu](https://github.com/jananitkt)
